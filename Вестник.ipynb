{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alier-une/CorpusFromPDF/blob/main/%D0%92%D0%B5%D1%81%D1%82%D0%BD%D0%B8%D0%BA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Описание задачи\n",
        "Необходимо создать корпус аннотаций, взятых из статьей по лингвистике, размещенных на сайте Вестник ЮУрГУ.\n"
      ],
      "metadata": {
        "id": "Iw27W3kV6OmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Установка необходимых библиотек"
      ],
      "metadata": {
        "id": "QHytttTUOuSQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CFF9XC5QKQqp"
      },
      "outputs": [],
      "source": [
        "# Устанавливаем библиотеку отправки всех видов HTTP-запросов\n",
        "!pip install requests --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8eUDZDrRKQaO"
      },
      "outputs": [],
      "source": [
        "# Устанавливаем библиотеку для парсинга HTML и XML документов\n",
        "!pip install beautifulsoup4 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlHV9wBSmqDG",
        "outputId": "9e5e50e8-95f9-45d3-d06c-6952259ccd55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (42.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Installing collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20231228\n"
          ]
        }
      ],
      "source": [
        "# Устанавливаем библиотеку для считывания файла PDF по пути репозитория\n",
        "!pip install PyPDF2 --quiet\n",
        "# Устанавливаем библиотеку для выполнения анализа структуры и извлечения текста и формата из PDF.\n",
        "!pip install pdfminer.six"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Импортирование зависимостей"
      ],
      "metadata": {
        "id": "P33ZMAXuO8kH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BHZdNkLhKO4k"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aLIB_zgwn2LO"
      },
      "outputs": [],
      "source": [
        "# Для считывания PDF\n",
        "import PyPDF2\n",
        "\n",
        "# Для анализа структуры PDF и извлечения текста\n",
        "from pdfminer.high_level import extract_pages, extract_text\n",
        "from pdfminer.layout import LTTextContainer, LTChar, LTRect, LTFigure"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Создание функции для извлечения текста из пдф-файла"
      ],
      "metadata": {
        "id": "bWtUsVP5PXbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_extraction(element):\n",
        "    # Извлекаем текст из вложенного текстового элемента\n",
        "    line_text = element.get_text()\n",
        "\n",
        "    # Находим форматы текста\n",
        "    # Инициализируем список со всеми форматами, встречающимися в строке текста\n",
        "    line_formats = []\n",
        "\n",
        "    for text_line in element:\n",
        "        if isinstance(text_line, LTTextContainer):\n",
        "            # Итеративно обходим каждый символ в строке текста\n",
        "            for character in text_line:\n",
        "                if isinstance(character, LTChar):\n",
        "                    # Добавляем к символу название шрифта\n",
        "                    line_formats.append(character.fontname)\n",
        "                    # Добавляем к символу размер шрифта\n",
        "                    line_formats.append(character.size)\n",
        "    # Находим уникальные размеры и названия шрифтов в строке\n",
        "    format_per_line = list(set(line_formats))\n",
        "\n",
        "    # Возвращаем кортеж с текстом в каждой строке вместе с его форматом\n",
        "    return (line_text, format_per_line)"
      ],
      "metadata": {
        "id": "X6hnqLwGay_8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Парсер сайта"
      ],
      "metadata": {
        "id": "5EL0I4ULPk64"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yBhK1W0fK1HM",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "start_url = 'https://vestnik.susu.ru/linguistics/issue/archive'\n",
        "\n",
        "# Получаем данные со стартовой страницы\n",
        "start_page = requests.get(start_url)\n",
        "# Изучаем данные со стартовой страницы\n",
        "soup = BeautifulSoup(start_page.text, 'html.parser')\n",
        "\n",
        "      #links = soup.find_all('a') находим все гиперссылки\n",
        "\n",
        "# Выбираем все гиперссылки\n",
        "link_elements = soup.select(\"a[href]\")\n",
        "\n",
        "# Создаем список для записи только необходимых ссылок на тома\n",
        "toms = []\n",
        "for link_element in link_elements:\n",
        "   url = link_element['href']\n",
        "   # Ищем нужные ссылки на тома по тексту на странице\n",
        "   if 'Том' in link_element.get_text():\n",
        "      # Добавляем найденные ссылки в список\n",
        "      toms.append(url)\n",
        "\n",
        "# Создаем список для записи только необходимых ссылок на пдф-файлы\n",
        "pdf_urls = []\n",
        "# Итерируем через созданный список toms (len(toms) ограничение, сколько томов нам надо)\n",
        "for i in range(len(toms[:3])):\n",
        "    # Получаем данные со страницы тома\n",
        "    tom_page = requests.get(toms[i])\n",
        "    # Изучаем данные со страницы тома\n",
        "    tom_soup = BeautifulSoup(tom_page.text, 'html.parser')\n",
        "    # Выбираем все гиперссылки\n",
        "    tom_links = tom_soup.select(\"a[href]\")\n",
        "\n",
        "    # Создаем список для записи только необходимых ссылок на статьи\n",
        "    articles = []\n",
        "    for tom_link in tom_links:\n",
        "        article_url = tom_link['href']\n",
        "        # Ищем нужные ссылки на статьи по тексту на странице\n",
        "        if 'PDF' in tom_link.get_text():\n",
        "          # Добавляем найденные ссылки в список\n",
        "          articles.append(article_url)\n",
        "\n",
        "    #Итерируем через созданный список articles (len(articles) ограничение, сколько пдф-файлов нам надо)\n",
        "    for j in range(len(articles)):\n",
        "        articles_page = requests.get(articles[j])\n",
        "        articles_soup = BeautifulSoup(articles_page.text, 'html.parser')\n",
        "        # Выбираем все гиперссылки\n",
        "        pdf_links = articles_soup.select(\"a[href]\")\n",
        "\n",
        "        for pdf_link in pdf_links:\n",
        "            pdf_file = pdf_link['href']\n",
        "            # Ищем нужные ссылки на пдф-файлы по тексту на странице\n",
        "            if 'Скачать этот файл PDF' in pdf_link.get_text():\n",
        "              # Получаем итоговый список с ссылками на пдф-файлы статьей\n",
        "              pdf_urls.append(pdf_link.get('href'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Формирование корпуса"
      ],
      "metadata": {
        "id": "gbvptE2aPybR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9n8La-G1nNXm"
      },
      "outputs": [],
      "source": [
        "# Создаем функцию, которая принимает на вход список ссылок и количество слов для извлечения\n",
        "def make_corpus(pdf_urls, max_words):\n",
        "  # Инициализируем счетчик слов для дальнейшего сравнения\n",
        "  total_words = 0\n",
        "  # Используем функцию enumerate(), которая возвращает объект в формате пар ключ-значение\n",
        "  for num, url in enumerate(pdf_urls, start=1):\n",
        "      # Извлекаем данные из определенного ресурса\n",
        "      response = requests.get(url)\n",
        "      # Выводим номер скачиваемого файла\n",
        "      print(\"Downloading file: \", num)\n",
        "      # Write content in pdf file\n",
        "      # Открываем пдф с указанием только для записи (бинарный)\n",
        "      pdf_file = open(\"pdf\"+str(num)+\".pdf\", 'wb')\n",
        "      # Записываем в файл наполнение (метод .content обеспечивает доступ к чистым байтам ответного пейлоада)\n",
        "      pdf_file.write(response.content)\n",
        "      # Закрываем файл\n",
        "      pdf_file.close()\n",
        "      # Выводим статус скачиваемого файла\n",
        "      print(\"File \", num, \" downloaded\")\n",
        "\n",
        "      # Обращаемся к необходимому файлу\n",
        "      pdf_path = f'/content/pdf{num}.pdf'\n",
        "      # Создаем список для записи текстовых элементов\n",
        "      text = []\n",
        "      for pagenum, page in enumerate(extract_pages(pdf_path)):\n",
        "          # Итеративно обходим элементы, из которых состоит страница\n",
        "          for element in page:\n",
        "              # Проверяем, является ли элемент текстовым\n",
        "              if isinstance(element, LTTextContainer):\n",
        "                  text.append(text_extraction(element))\n",
        "\n",
        "      # Итеративно обходим созданный список с текстовыми элементами\n",
        "      for k in range(len(text)):\n",
        "        # Задаем условие для поиска аннотаций\n",
        "        if text[k][0].startswith('Аннотация'):\n",
        "          # Выбираем элемент по заданному условию\n",
        "          annotation = text[k][0]\n",
        "          # Задаем условие на минимальное количество слов в аннотации\n",
        "          if len(annotation.split()) > 100:\n",
        "            # Создаем конечный файл \"Корпус\"\n",
        "            with open('Корпус.txt', 'a') as file:\n",
        "              # Записываем в файл аннотацию, убирая симоволы переноса слова и переноса строки\n",
        "              file.write(annotation.replace('-\\n', '').replace('\\n', '') + '\\n')\n",
        "              # Добавляем к счетчику количество слов в аннотации\n",
        "              total_words += len(annotation.split())\n",
        "              # Сравниваем показатель счетчика с желаемым максмальным количеством слов\n",
        "              if total_words >= max_words:\n",
        "                return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gk50hdcrYfP",
        "outputId": "58b1eb26-1034-4d5f-be13-9b6d89f373be",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading file:  1\n",
            "File  1  downloaded\n",
            "Downloading file:  2\n",
            "File  2  downloaded\n",
            "Downloading file:  3\n",
            "File  3  downloaded\n",
            "Downloading file:  4\n",
            "File  4  downloaded\n"
          ]
        }
      ],
      "source": [
        "# Задаем необходимое количество слов\n",
        "max_words = 500\n",
        "# Вызываем функцию и передаем список с ссылками и необходимое количество слов\n",
        "make_corpus(pdf_urls, max_words)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1_NaS4sYWRTx3x5RkfQGsay6pDlDuHvcb",
      "authorship_tag": "ABX9TyOc/5LEMNo2gWAphk1LxlK/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}